# １週間で機械学習の初歩を勉強してみた
2022年2月21日～の週で勉強したことをまとめます。  
この週は機械学習（初歩）について勉強しました。  

かなりダラダラと書いていますが、`1.`と`2.`だけ読んでいただければ、自分が何を勉強していたのかは大体わかってもらえるかと思います。  
3. 以降は学んだことを羅列しているだけなので、興味ない人は`1.`と`2.`だけ見ていってください。

## １．何の開発・勉強を行っていたか
- 機械学習の概要・歴史  
現在AIブームは３回目を迎えていますが、１回目と２回目がどのようにしてブームが起き、そして去ったかを学びました。

- ディープラーニングについて  
やりたいことは自然言語処理です。ただ自然言語処理はディープラーニングを原理とした技術なので、  
まずはその原理を理解するためにディープラーニングについて勉強しました。


## ２．読んだ書籍、活用したWebサイト
**書籍**
- [ゼロから作るDeepLearning](https://www.amazon.co.jp/gp/product/4873117585/ref=dbs_a_def_rwt_hsch_vapi_taft_p1_i0)  
自然言語処理の本というわけではなく、あくまでディープラーニングの本。  
自然言語処理はディープラーニングをベースとする技術なので、まずはディープラーニングを勉強するために読みました。  

- [人工知能は人間を超えるか](https://www.amazon.co.jp/gp/product/B00UAAK07S/ref=dbs_a_def_rwt_hsch_vapi_tkin_p1_i1)
人工知能の歴史を軽く知るのも大事かと思い、BOOKOFFで220円で購入。  
人工知能ブームが過去に起こった時に、何が原因でそのブームが去っていったのかを、人工知能研究者の視点から語られているので面白いです。
ディープラーニングが世界を変えるかもしれないとのこと。

- [Pythonではじめる機械学習](https://www.amazon.co.jp/gp/product/4873117984/ref=dbs_a_def_rwt_hsch_vapi_taft_p1_i0)  
Pythonで機械学習をはじめるための書籍。
サンプルコードが豊富に乗っており、それに沿って手を動かしながら機械学習の初歩を学習できる（のだと思う）。
軽く読んだだけでまだ手を動かしてやれてないです。来週はこの本に集中して取り組む予定です。

**サイト**
- [【保存版・初心者向け】独学でAIエンジニアになりたい人向けのオススメのAI勉強方法 (2019年改定版)](https://qiita.com/tani_AI_Academy/items/ab2208e21a2215f8dfad)  
[Kazunori Tani](https://twitter.com/tankazunori0914)さんという、**AI Academy** という会社の方が書かれた記事。  
AIエンジニアになるためのロードマップ的なものが解説されており、非常に参考になりました。  
とりあえずこの記事にあるロードマップに従って勉強をしていく予定です。

- [現役シリコンバレーエンジニアが教えるPython 3 入門 + 応用 +アメリカのシリコンバレー流コードスタイル](https://www.udemy.com/course/python-beginner/)  
Pythonや開発環境のPyCharmについて教えてくれる講座。  
入門編だけやってまだ応用編はやれていないですが、わかりやすいのでオススメです。

## ３．実際に学んだこと
ここでは、自分が本を読みながら線を引いた個所について章ごとに紹介していきます。  
本の内容を分かりやすくまとめるということはしません。

### ゼロから作るDeepLearning
DeepLearning本で学んだ内容について解説している記事はこちらを参照。  
https://qiita.com/segavvy/items/4e8c36cac9c6f3543ffd

#### １章 Python入門
自分はPythonを触ったことはあったのでここは軽く読み飛ばしました。

#### ２章 パーセプトロン
##### パーセプトロンとは
パーセプトロンは複数の信号を受け取って、一つの信号を出力するモジュールです。  
入力信号を**x1**,**x2**、出力信号を**y**として、重みを**w1**,**w2**とすると、次のような数式が成り立ちます。  
```
    y = 0  (w1x1 + w2x2 <= θ)  
        1  (w1x1 + w2x2 <  θ)  
```
つまり、w1x1 + w2x2 がある閾値（θ）を超えると出力が１になり、それ以外は０になります。  
**1**の状態は信号を流し、**0**の状態は信号を流しません。

#### ３章 ニューラルネットワーク
##### ニューラルネットワークとは
先ほどのパーセプトロンの重み（w1, w2）の値は、人間がどのような値にするかを設定しています。  
この重みをデータから自動で推定するのがニューラルネットワークです。

パーセプトロンの場合、出力は**0**か**1**かの2択でしたが、ニューラルネットワークの場合は出力は連続的な値になります。  
つまり0と1の間にある**0.5**とか**0.32**みたいな数値が出力されうるということです。

##### 非線形関数
入力信号の総和を出力に変換する関数が**活性化関数**です。  
ニューラルネットワークの活性化関数には**ステップ関数**や**シグモイド関数**が使用されますが、これらは非線形特性を有します。  
ニューラルネットワークは層を深くすることでパラメータの学習能力を向上させていきますが、  
活性化関数に線形関数を用いると、この層を深くする意味がなくなってしまうという問題があります。
これは、線形関数で作られたネットワークをいくら深くしても、同じことができる１層しかないネットワークで置き換えられてしまう、という点に起因します。

数式で説明すると線形関数 h(x)=cx を活性化関数として、 y(x)=h(h(h(x))) を行う計算を３層のネットワークに対応させてみます。  
h(x)=cx を y(x) の式に代入すると y(x)=c×c×c×x となりますが、これは y(x)=ax という１回の掛け算に置き換えられます。

##### ReLU（Rectified Linear Unit)
活性化関数としてシグモイドは古くから利用されてきましたが、最近は ReLU という関数が主に使われるそうです。  
ReLU は x が 0 以下の時は常に 0、x が 0 より大きいときは x を出力する関数です。

##### 出力層の活性化関数
扱う問題によって使用される活性化関数も変わります。下記の表が一般的な使われ方のようです。
| 問題 | 活性化関数 |
----|---- 
| 回帰問題 | 恒等関数 |  
| ２クラス分類問題 | シグモイド関数 |  
|多クラス分類|ソフトマックス関数|  

##### ソフトマックス関数実装上の注意
ソフトマックス関数では各出力値を、出力すべての総和で割るという計算を行います。  
その際、出力値の総和を計算するとオーバーフローを起こしてしまう可能性があります。  
その対策として、出力値の最大値を抽出して、その値ですべての出力値を引いていくということをします。

##### ソフトマックス関数の特徴
出力値をソフトマックス関数にかけても、各値の大小関係に変化はありません。  
なので、ニューラルネットワークの分類問題を扱う場合などは、ソフトマックス関数は省略されます。


#### ４章 ニューラルネットワークの学習
##### ニューラルネットワークが学習するとは
ニューラルネットワークでは学習のために損失関数という指標を導入します。  
この損失関数を基準として、その値が最も小さくなる重みパラメータを探し出すと言ことが学習の目的です。

##### 機械学習とニューラルネットワークの違い
機械学習では、データの中から規則性を見つけるというアプローチが行われます。  
その際、データ（例えば画像）はベクトルの形に変形されてから学習モジュールにかけられます。  
つまり、機械学習では人間がデータの特徴を考えて、機会に教えてあげる必要があります。

ニューラルネットワークでは、データをそのまま学習します。  
つまり、データに含まれる特徴も機会が学習して獲得していきます。

##### 汎化能力
学習では大量の訓練データを使いますが、汎化能力は訓練データではなく、まだ見ぬデータに対する能力で測られます。  
なので、特定のだれかが書いた数字を認識する能力は全く役に立ちません。  
それよりも、大勢の人が書いた様々な形式で書かれた数字を認識することができる能力を獲得する必要があります。

##### ミニバッチ学習
訓練データが10,000枚あったとして、その中から100枚を選んで学習することを**ミニバッチ学習**という。  
テレビが視聴率を測定する方法もこのミニバッチ学習のような方法をとっている。

##### なぜ損失関数を設定するのか
ニューラルネットワークでは損失関数ができるだけ小さくなるようにパラメータを調整します。  
そこでパラメータの微分を計算して、その微分を手掛かりに少しづつパラメータを調整していきます。

認識精度を指標にしてはいけないのは、微分がほとんどの場所で**0**になってしまうからです。  
同じ理由で、ニューラルネットワークの活性化関数にステップ関数を用いることができません。

##### 勾配法
微分を求めるとは、その地点における勾配（傾き）を求めるということです。  
この傾きが示す方向に沿って進んでいけば、いずれは傾きがない場所に辿り着ける可能性があります。  

この時、傾きに沿って進む度合いを調整するのが**学習率です**  
この学習率を大きくしすぎると変化量が大きくなりすぎて、パラメータは無限大へと発散してしまいます。  
逆に小さすぎると変化量が小さすぎて、いつまでたっても学習が進まないという状態になってしまいます。


#### ５章 誤差逆伝播法
##### 誤差逆伝播法とは
重みパラメータの勾配の計算を効率よく計算する方法です。  
ニューラルネットワークでは膨大な量の計算が行われるので、計算時間を短くする方法がほぼ必須です。  
プログラムで関数の微分を求める方法として**数値微分**がありますが、こちらは計算時間が掛かりすぎてしまうという問題があります。

誤差逆伝播法では全体の傾きはひとまず考えず、局所的な勾配を計算することで計算時間を短縮する手法です。
この局所的な勾配を次の層へと流していくため、**伝播**法というわけです。

##### 活性化関数の逆伝播時の挙動
**ReLU**では、順伝播時の入力値xが0より大きいなら、逆伝播時には上流から下流へと値をそのまま流します。
入力値xが0以下であれば、上流からの値はそこでストップさせ、下流には0を流します。つまりそこで流れが途絶えます。

Softmax-with-Loassレイヤでは、逆伝播時の結果は **(y1-t1, y2-t2, y3-t3)** というような綺麗な結果になります。  
ここでyはソフトマックスレイヤの出力、tは教師ラベルのことです。  
**y-t**というのは出力値と正解との差分になっています。この誤差が大きければ、逆伝播時にも大きな値が流れていきます。  
結果として、学習する内容が大きくなるので、より多く学ぶことになります。  
逆に誤差が少なければ学習量は小さくなりますが、誤差が少ないということはそもそも学ぶ必要があまりないということなので問題ありません。

##### 数値微分の活用法
誤差逆伝播法に比べると数値微分は計算時間が掛かりすぎるという問題がありました。  
では数値微分は全く必要ないのかというと、そんなことはありません。  
誤差逆伝播法の実装の正しさを確認するために、数値微分は活用されます。

誤差逆伝播法は複雑な実装になりやすいのに対して、数値微分はとてもシンプルな手法なのでミスが起きにくいです。  
そこで、誤差逆伝播法の実装がミスしていないか確認するために、数値微分で再度勾配を計算して確かめます。


#### ６章 学習のテクニック
##### SGD（stochastic gradient descent）の欠点
SGD：確率的勾配降下法  
勾配を計算して、その方向へと徐々に移動していくような手法であるSGDの欠点として、
その地点における勾配がかならずしも最小値の方向を向いていないという問題があります。

例えば `f(x,y) = 1/20 x^2 + y^2` という関数を考えます。  
この関数の勾配をイメージすると、**y**方向は普通にある程度傾くことが想像できますが、**x**方向はかなり微小な傾きになってしまいます。  
つまり学習の過程で、x軸とy軸の学習量に大きな開きができてしまい、x軸の学習の進みはとてもゆっくりとしたものになります。  
こうなると、学習経路はかなり非効率なものとなり、結果として無駄な計算を多く含んでしまうということになります。

##### Momentum
Momentumはパラメータの学習に物理法則を取り込んだ手法です。  
パラメータの更新式に「抵抗力」に相当するものを取り込むことで、変化量の多いパラメータに対して制限をかけます。  
すると、先ほどの例の x にはあまり抵抗が発生しないのに対し、y に対しては大きな抵抗が働きます。  
これにより、パラメータ同士での学習力の差を緩和しているのだと思います。

##### AdaGrad
パラメータの学習係数を減衰させる手法です。  
パラメータごとに学習係数を設定し、よく学んだパラメータの学習率を減衰させます。

##### RMSProp
AdaGradは過去の勾配をすべて記録していくため、学習が進むほど更新度合いが少なくなります。  
無限に学習していくとそのうち更新量が0になり、全く学習しなくなります。

RMSPropでは過去のすべての勾配を均一に加算していくのではなく、徐々に忘れて新しい勾配の情報が大きく反映されるように加算します。

##### Adam
Momentum と AdaGrad を融合させたような手法です。

##### 重みの初期値
重みの初期値を0にしてはいけません。正確には重みを均一な値に設定してはいけません。  
理由は、すべての重みの値が同じように更新されてしまうからです。  
全てのニューロンが同じような動作をしても、複雑な問題を解く能力は得られない、というのは何となくイメージしやすいかと思います。

##### 隠れ層の出力値
隠れ層の出力が0と1に偏ると、逆伝播時に値がほとんど伝わらなくなります。これを**勾配消失**といいます。  

##### 重みの初期値の理想
扱う活性化関数によって初期値を変える。具体的には下記の通り。

ReLU：**Heの初期値**  
sigmoidやtanhなどのS字カーブ：**Xavierの初期値**

##### Batch Normalization
とりあえずこれをやっておけば初期値にあまり悩む必要がなくなる。すでに色んなところで使用されている技術。

##### 過学習
過学習が起こる原因は下記の2つ
- パラメータがたくさんある表現力の高いモデル
- 訓練データが少ない

訓練データとテストデータの認識精度に大きな隔たりがある場合、過学習が起きている可能性が高い。

##### Weight decay
大きな重みをもつことに対してペナルティを課すことで、過学習を抑制する手法。

##### Dropout
ニューロンをランダムに消去しながら学習する手法。これも過学習を抑制する方法の一つ。


#### ７章 畳み込みニューラルネットワーク
##### 畳み込み層
畳み込みニューラルネットワークでは、各層を流れるデータが形状を持つようになる。例えば３次元になったりする。  
逆に言えば、これまでの全結合層ではデータの形状はすべて無視されてしまっていた。

データの形状は画像のようなもの。画像はたくさんのピクセルの集まりだが、ピクセル同士の距離には意味がある。  
近いピクセル同氏は似た色になるとか、遠いピクセル同士だとあまり関係がないなど。

##### パディング
畳み込み演算ではその計算の都合上、普通に計算してしまうとどんどん情報量が少なくなっていく。  
最終的にはサイズが１になってしまい、それ以上畳み込み演算が適用できなくなる。  

パディングとは画像データの外側に値0のデータを埋めていって、サイズを大きくする処理。これにより入力データのサイズを大きくすることができる。

##### 畳み込み演算はブロックをイメージする
畳み込み演算ではデータに形状があるので、ブロックの塊をイメージするとわかりやすい。  
入力データとフィルターのチャンネル数は一致させないといけない。  
フィルターの個数をＮ個とすると、畳み込み演算後のデータのチャネル数はＮとなる。  
またバイアスはチャンネルごとにただ一つだけのデータを持つ。

##### プーリング層
プーリング層は縦・横の形状を小さくする演算。  
一般的にプーリングサイズとストライドは同じ値に設定する。  
※ プーリングサイズ＞ストライド となると、MAXプーリングなどでは同じ要素が抽出されてしまう可能性がある。

##### im2colによる展開
PythonのNumpyではfor文を使うと処理が遅くなってしまう。  
畳み込み演算ではデータに形状があるため、for文で処理したくなるが、それだと処理時間が掛かってしまう。  
そこで、**im2col**により形状のあるデータを１次元データに展開するということを行う。

##### 画像認識
画像認識分野では畳み込み層とプーリング層は必須のモジュールとなっている。  
CNNでは画像という形状のあるデータをうまく取り込める。

##### 階層構造による情報抽出
ディープラーニングを可視化するという研究がある。  
この研究によると、層が深くなるにしたがって抽出される情報がより抽象的になることがわかっている。  
最初のほうの層で単純な形状を読み取り、最終的にはモノの「意味」を理解するように抽出対象が変化していく。


#### ８章 ディープラーニング
##### 層を深くする
ディープラーニングでは層を深くするほど表現力が増す。  
パラメータが少なくても層を深くすることで、充分な表現力を得ることができる。
例えば`5×5`の畳み込み演算1回は、`3×3`の演算2回でカバーできる。
また層を深くすることで学習パラメータを少なくできるのも一つの利点。

層を深くすることで、学習すべき問題を階層化に分解できる。
つまり各層が理解すべき問題がよりシンプルになるため、少ないデータで効率よく学習できる。


## ４．開発したものの紹介




## ５．可能ならソースコードを載せて解説




## ６．今後何をしたいのかの展望




## ７．まとめ



